{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea for this network, is to take into account the fact that the car stays flat on the road. Extended control when driving on two wheels only with the car on its side, does not seem necessary. The image area will be cut in two stripes: \n",
    "\n",
    "* the distant part, which wiggles as the road turns, and is mostly useful to predict the long term average steering order;\n",
    "* the close area, which begins after the hood, and moves in a more geometric fashion is response to car motion;\n",
    "\n",
    "In the close area, the usual machine vision network architectures will be used: 2D convolution layers to extract patterns and features, and dense layers similar to the NVidia network. \n",
    "\n",
    "* Image normalization;\n",
    "* Convolution layer, valid padding, 5x5 kernel, stride 2, 24 layers;\n",
    "* Convolution layer, valid padding, 5x5 kernel, stride 2, 36 layers;\n",
    "* Convolution layer, valid padding, 5x5 kernel, stride 2, 48 layers;\n",
    "* Convolution layer, valid padding, 3x3 kernel, stride 1, 64 layers;\n",
    "* Convolution layer, valid padding, 3x3 kernel, stride 1, 64 layers, at which point the map is 1 pixel high;\n",
    "* Flatten; \n",
    "* Dense layer 100 neurons;\n",
    "* Dense layer 50 neurons;\n",
    "* Dense layer 10 neurons;\n",
    "* Output 1 neuron, which controls steering;\n",
    "\n",
    "The image normalization will separate brightness and color information. The brightness will be computed as in the grayscale function, while color information will consist in a one_hot_encoded color class among the 6 primary colors, red, green, blue, cyan, yellow, magenta, plus gray, and saturation information derived as max(r,g,b)-min(r,g,b). The brightness and saturation data are normalized to [-1;+1]. The image preprocessing will be done in the training, and drive.py will be modified to provide image preprocessing as well.\n",
    "\n",
    "The smaller distant part will be preprocessed in the same way as the close area. However, a motion compensation algorithm will remove the background by substraction from the memorized previous picture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 24432 entries from driving log\n",
      "['/home/jm/Projets/Udacity/CarND-Behavioral-Cloning-P3/data/IMG/center_2017_08_31_19_21_21_277.jpg', '/home/jm/Projets/Udacity/CarND-Behavioral-Cloning-P3/data/IMG/left_2017_08_31_19_21_21_277.jpg', '/home/jm/Projets/Udacity/CarND-Behavioral-Cloning-P3/data/IMG/right_2017_08_31_19_21_21_277.jpg', '-0.0472103', '1', '0', '30.19018']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "lines = []\n",
    "\n",
    "# Read lines from log file to retrieve path of center, left and right images\n",
    "# Note that the csv file is incorrectly generated in locales which use comma as a decimal point.\n",
    "# The simulator must run in en_US or C locale to get a correct csv file.\n",
    "with open('data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        lines.append(line)\n",
    "\n",
    "print('Read '+str(len(lines))+' entries from driving log')\n",
    "# Visual check that decimal numbers are actually read from the file. If the file is incorrect, only integers\n",
    "# will be shown, and the number of columns will not be 7.\n",
    "print(lines[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66105 samples found\n",
      "['/home/jm/Projets/Udacity/CarND-Behavioral-Cloning-P3/data/IMG/center_2017_08_31_19_21_10_640.jpg', '/home/jm/Projets/Udacity/CarND-Behavioral-Cloning-P3/data/IMG/center_2017_08_31_19_21_10_716.jpg', 0.0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "# Augment training data using left and right cams, with steering angle offset\n",
    "# and flipping all images (and reversing the sign of the steering order)\n",
    "# This should give us around 60k images for training.\n",
    "# Note that steering angle correction for a side camera is -2 e x/(V²t²)\n",
    "# where e is the distance between the rear axle and the cameras, x is the lateral offset (x axis) of the camera\n",
    "# (x<0 for left cam, x>0 for right cam), V and t are current speed and time to rejoin trajectory, and they are\n",
    "# tuning parameters. Maximum speed is around 30 mph (13 m/s) and at high speed, 2 seconds to rejoin the ideal\n",
    "# trajectory seems okay. At a slower speed, we can take more time, so operating at constant Vt makes sense.\n",
    "# We shall therefore take Vt=26, e=2.61m, x=-0.806m for the left cam and x=+0.825 for the right cam.\n",
    "# The latter values are extracted from the Unity code of the simulator.\n",
    "def augment_data(lines):\n",
    "    \"\"\"\n",
    "    Takes as input the content of the csv file, and produces two lists: image_paths and steering angles.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    measurements = []\n",
    "    k = -2*2.61/(26^2)\n",
    "    \n",
    "    for line in lines:\n",
    "        # Only images where speed > 0 are added\n",
    "        speed = float(line[6])\n",
    "        if speed > 0:\n",
    "            angle = float(line[3])\n",
    "            # Only add if file exists\n",
    "            # For each recorded file path original and final location are tested\n",
    "            path = re.sub(r\"/data\\d+/\",\"/data/\",line[0])\n",
    "            if os.access(line[0], os.R_OK):\n",
    "                image_paths.append(line[0])\n",
    "                measurements.append(angle)\n",
    "            elif os.access(path, os.R_OK):\n",
    "                image_paths.append(path)\n",
    "                measurements.append(angle)\n",
    "            path = re.sub(r\"/data\\d+/\",\"/data/\",line[1])\n",
    "            if os.access(line[1], os.R_OK):\n",
    "                image_paths.append(line[1])\n",
    "                measurements.append(angle-0.806*k)\n",
    "            elif os.access(path, os.R_OK):\n",
    "                image_paths.append(path)\n",
    "                measurements.append(angle-0.806*k)\n",
    "            path = re.sub(r\"/data\\d+/\",\"/data/\",line[2])\n",
    "            if os.access(line[2], os.R_OK):\n",
    "                image_paths.append(line[2])\n",
    "                measurements.append(angle+0.825*k)\n",
    "            elif os.access(path, os.R_OK):\n",
    "                image_paths.append(path)\n",
    "                measurements.append(angle+0.825*k)\n",
    "    \n",
    "    return image_paths, measurements\n",
    "\n",
    "samples = []\n",
    "\n",
    "cam_paths, angles = augment_data(lines)\n",
    "\n",
    "# Previous image frame is used to cancel out fixed background using a motion compensation algorithm\n",
    "# Because images have been added three by three, the previous frame for each cam is three indices before in the list\n",
    "# The first three frames are discarded: they are used as prior image for the next one\n",
    "previous_frame = cam_paths[:-3]\n",
    "cam_paths = cam_paths[3:]\n",
    "angles = angles[3:]\n",
    "\n",
    "# Assemble as one list\n",
    "samples = [ [i,j,k] for i,j,k in zip(previous_frame, cam_paths, angles)]\n",
    "# All intermediate variables are cleared\n",
    "del cam_paths, angles, previous_frame\n",
    "print(len(samples),'samples found')\n",
    "print(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.sort()\n",
    "print(samples[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 20% of images to use as validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "samples_train, samples_valid = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "print('Feature set shape:',samples_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from random import shuffle\n",
    "\n",
    "# Define a generic generator\n",
    "def generator(samples, batch_size=64):\n",
    "    batch_size//=2  # All images are flipped...\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            prevs  = []\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                #prev  = cv2.imread(batch_sample[0])\n",
    "                # load and trim image to only see section with road\n",
    "                image = cv2.imread(batch_sample[1])[70:135].astype(np.float32)\n",
    "                # image preprocessing: resize, colorspace conversion\n",
    "                #image = cv2.cvtColor(image/255.0, cv2.COLOR_RGB2HSV)\n",
    "                angle = float(batch_sample[2])\n",
    "                # append to batch\n",
    "                #prevs.append(prev)\n",
    "                images.append(image)\n",
    "                angles.append(angle)\n",
    "                images.append(cv2.flip(image,1))\n",
    "                angles.append(-angle)\n",
    "\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, Lambda\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Create NVidia-like neural network\n",
    "# Input image trimmed 70 from top, 25 from bottom : shape=(65,320,3)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Lambda(lambda x: x/255.0 - 0.5 ,input_shape=(65,320,3), output_shape=(65,320,3)))\n",
    "print(model.output_shape)\n",
    "model.add(Convolution2D(24, 5, 5, border_mode='valid', subsample=(2,2), activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(Convolution2D(36, 5, 5, border_mode='valid', subsample=(2,2), activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(Convolution2D(48, 5, 5, border_mode='valid', subsample=(2,2), activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='valid', activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='valid', activation='relu'))\n",
    "print(model.output_shape)\n",
    "model.add(Flatten())\n",
    "print(model.output_shape)\n",
    "#model.add(Dropout(p=0.5))\n",
    "#print(model.output_shape)\n",
    "model.add(Dense(100))\n",
    "print(model.output_shape)\n",
    "model.add(Dense(50))\n",
    "print(model.output_shape)\n",
    "model.add(Dense(10))\n",
    "print(model.output_shape)\n",
    "model.add(Dense(1))\n",
    "print(model.output_shape)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print('Keras model ready for training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from random import shuffle\n",
    "\n",
    "# Define a generator which separates far field and near field\n",
    "def generator(samples, batch_size=64):\n",
    "    batch_size//=2  # All images are flipped...\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            #prevs  = []\n",
    "            images_far = []\n",
    "            images_near= []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                #prev  = cv2.imread(batch_sample[0])\n",
    "                # load and trim image to only see section with road\n",
    "                image = cv2.imread(batch_sample[1])\n",
    "                far = image[50:90].astype(np.float32)\n",
    "                near = image[90:130].astype(np.float32)\n",
    "                # image preprocessing: resize, colorspace conversion\n",
    "                far = cv2.resize(far,(80,10),interpolation=cv2.INTER_AREA)\n",
    "                far = cv2.cvtColor(far/255.0, cv2.COLOR_RGB2HSV)\n",
    "                near = cv2.resize(near,(160,20),interpolation=cv2.INTER_AREA)\n",
    "                near = cv2.cvtColor(near/255.0, cv2.COLOR_RGB2HSV)\n",
    "                angle = float(batch_sample[2])\n",
    "                # append to batch\n",
    "                #prevs.append(prev)\n",
    "                images_far.append(far)\n",
    "                images_near.append(near)\n",
    "                angles.append(angle)\n",
    "                images_far.append(cv2.flip(far,1))\n",
    "                images_near.append(cv2.flip(near,1))\n",
    "                angles.append(-angle)\n",
    "\n",
    "            X1_train = np.array(images_far)\n",
    "            X2_train = np.array(images_near)\n",
    "            y_train = np.array(angles)\n",
    "            X1,X2,y = sklearn.utils.shuffle(X1_train, X2_train, y_train)\n",
    "            yield ([X2,X1],y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, Dropout, Merge\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Create my neural network\n",
    "# Far field view recognizes wiggling line as the track in the distance\n",
    "model_far = Sequential()\n",
    "model_far.add(BatchNormalization(mode=1,input_shape=(10,80,3)))\n",
    "model_far.add(Flatten())\n",
    "model_far.add(Dense(200,activation='relu'))\n",
    "model_far.add(Dropout(p=0.5))\n",
    "model_far.add(Dense(100))\n",
    "print(model_far.output_shape)\n",
    "\n",
    "model_near = Sequential()\n",
    "model_near.add(BatchNormalization(mode=1,input_shape=(20,160,3)))\n",
    "model_near.add(Convolution2D(16, 5, 5, border_mode='valid', subsample=(2,2), activation='relu'))\n",
    "model.add(Dropout(p=0.1))\n",
    "model_near.add(Convolution2D(36, 5, 5, border_mode='valid', subsample=(2,2), activation='relu'))\n",
    "model.add(Dropout(p=0.2))\n",
    "model_near.add(MaxPooling2D())\n",
    "model_near.add(Flatten())\n",
    "model_near.add(Dense(100,activation='relu'))\n",
    "model_near.add(Dropout(p=0.5))\n",
    "print(model_near.output_shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([model_near,model_far],mode='concat',concat_axis=1))\n",
    "print(model.output_shape)\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(1))\n",
    "print(model.output_shape)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print('Keras model ready for training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(next(train_generator)[0])==2:  # Because it is a list of two numpy arrays\n",
    "    model_file = 'model2.h5'\n",
    "    nbe = 2\n",
    "else:\n",
    "    model_file = 'model.h5'\n",
    "    nbe = 15\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 192\n",
    "\n",
    "# Define generators\n",
    "train_generator = generator(samples_train, batch_size = batch_size)\n",
    "validation_generator = generator(samples_valid, batch_size = batch_size)\n",
    "\n",
    "# Train model and save with weights values\n",
    "# Number of samples is twice array length, because all images are also presented flipped for training.\n",
    "history_object = model.fit_generator(train_generator, samples_per_epoch = 2*len(samples_train), \n",
    "                    validation_data=validation_generator, nb_val_samples = 2*len(samples_valid),\n",
    "                    nb_epoch=nbe)\n",
    "model.save(model_file)\n",
    "print(model_file+' saved.')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain with more pictures instead of starting with random weights\n",
    "from keras.models import load_model\n",
    "\n",
    "# model_file is the best known model\n",
    "file = model_file\n",
    "file = model_file+'.tuned'\n",
    "nbe = 15    # Epochs already done\n",
    "epochs = 1\n",
    "batch_size = 192\n",
    "lock = 6          # 6 to lock all convolutional layers in NVidia architecture\n",
    "\n",
    "model = load_model(model_file)\n",
    "#model = load_model(model_file+'.tuned')\n",
    "\n",
    "print('The following layers are non trainable:')\n",
    "for layer in model.layers[:lock]:\n",
    "    print(layer.name)\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# Define generators\n",
    "train_generator = generator(samples_train, batch_size = batch_size)\n",
    "validation_generator = generator(samples_valid, batch_size = batch_size)\n",
    "\n",
    "# Re-train\n",
    "history_object = model.fit_generator(train_generator, samples_per_epoch = 2*len(samples_train), \n",
    "                    validation_data=validation_generator, nb_val_samples = 2*len(samples_valid),\n",
    "                    nb_epoch=epochs+nbe, initial_epoch=nbe)\n",
    "\n",
    "model.save(model_file+'.tuned')\n",
    "print(model_file+'.tuned saved.')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(history_object.history.keys())\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
